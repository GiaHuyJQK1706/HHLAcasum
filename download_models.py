from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers.utils import logging as hf_logging
import os
import sys
import shutil

# Táº¯t logging Ä‘á»ƒ gá»n gÃ ng hÆ¡n
hf_logging.set_verbosity_warning()

# Danh sÃ¡ch models cáº§n táº£i
MODELS = {
    # "facebook/bart-base": ("BART Base", "558MB"),
    # "t5-base": ("T5 Base", "892MB"),
    # "google/pegasus-xsum": ("PEGASUS XSum", "2.3GB"),
    # "allenai/led-base-16384": ("LED Base (long doc)", "500MB"),
    # "VietAI/vit5-base": ("VietAI viT5", "1.2GB"),
    "google/mt5-base": ("mT5 Base", "1.1GB"),
}

def print_progress(current, total, prefix='', suffix=''):
    """In thanh progress bar"""
    bar_length = 40
    filled = int(bar_length * current / total)
    bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
    percent = 100 * current / total
    print(f'\r{prefix} |{bar}| {percent:.1f}% {suffix}', end='', flush=True)
    if current == total:
        print()

def download_model(model_name, display_name, size, current_num, total_num):
    """Táº£i 1 model vá» mÃ¡y - TRá»°C TIáº¾P vÃ o ./models/"""
    print(f"\n{'='*60}")
    print(f"ğŸ“¥ [{current_num}/{total_num}] Äang táº£i: {display_name}")
    print(f"ğŸ”— Model: {model_name}")
    print(f"ğŸ’¾ KÃ­ch thÆ°á»›c: ~{size}")
    print(f"{'='*60}")
    
    try:
        # Táº¡o tÃªn folder an toÃ n
        safe_name = model_name.replace("/", "_")
        # QUAN TRá»ŒNG: DÃ¹ng Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i
        base_dir = os.path.dirname(os.path.abspath(__file__))
        save_path = os.path.join(base_dir, "models", safe_name)
        
        # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
        os.makedirs(save_path, exist_ok=True)
        
        # Kiá»ƒm tra Ä‘Ã£ táº£i chÆ°a
        config_file = os.path.join(save_path, "config.json")
        model_file = os.path.join(save_path, "pytorch_model.bin")
        
        if os.path.exists(config_file) and os.path.exists(model_file):
            print(f"âš ï¸  Model Ä‘Ã£ tá»“n táº¡i táº¡i {save_path}")
            choice = input("   Táº£i láº¡i? (y/n): ")
            if choice.lower() != 'y':
                print("   â­ï¸  Bá» qua model nÃ y")
                return True
        
        # PhÆ°Æ¡ng phÃ¡p má»›i: Táº£i vÃ o cache rá»“i MOVE sang ./models/
        print("â³ [1/3] Äang táº£i tokenizer tá»« Hugging Face...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        print("   âœ… Tokenizer hoÃ n thÃ nh")
        
        print("â³ [2/3] Äang táº£i model weights tá»« Hugging Face...")
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        print("   âœ… Model weights hoÃ n thÃ nh")
        
        # LÆ°u TRá»°C TIáº¾P vÃ o ./models/ (khÃ´ng qua cache)
        print(f"â³ [3/3] Äang lÆ°u vÃ o {save_path}...")
        tokenizer.save_pretrained(save_path)
        model.save_pretrained(save_path)
        print("   âœ… LÆ°u file hoÃ n thÃ nh")
        
        # Kiá»ƒm tra kÃ­ch thÆ°á»›c thá»±c táº¿
        total_size = 0
        for root, dirs, files in os.walk(save_path):
            for f in files:
                file_path = os.path.join(root, f)
                if os.path.exists(file_path):
                    total_size += os.path.getsize(file_path)
        actual_size_mb = total_size / (1024 * 1024)
        
        print(f"âœ… ÄÃ£ lÆ°u thÃ nh cÃ´ng!")
        print(f"   ğŸ“‚ ÄÆ°á»ng dáº«n: {save_path}")
        print(f"   ğŸ’¾ KÃ­ch thÆ°á»›c thá»±c: {actual_size_mb:.1f}MB")
        
        # Giáº£i phÃ³ng bá»™ nhá»›
        del model
        del tokenizer
        
        return True
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  ÄÃ£ há»§y bá»Ÿi ngÆ°á»i dÃ¹ng")
        sys.exit(0)
        
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")
        print(f"   ğŸ’¡ Tip: Kiá»ƒm tra káº¿t ná»‘i máº¡ng vÃ  thá»­ láº¡i")
        return False

def main():
    print("â•”" + "="*58 + "â•—")
    print("â•‘" + " "*18 + "ğŸš€ Táº¢I MODELS" + " "*27 + "â•‘")
    print("â•š" + "="*58 + "â•")
    
    # Hiá»ƒn thá»‹ Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i
    base_dir = os.path.dirname(os.path.abspath(__file__))
    models_dir = os.path.join(base_dir, "models")
    
    print(f"\nğŸ“‚ ThÆ° má»¥c lÆ°u: {models_dir}")
    print(f"ğŸ“Š Sá»‘ lÆ°á»£ng models: {len(MODELS)}")
    
    # Hiá»ƒn thá»‹ danh sÃ¡ch
    print("\nğŸ“‹ Danh sÃ¡ch models sáº½ táº£i:")
    for i, (model_name, (display_name, size)) in enumerate(MODELS.items(), 1):
        print(f"   {i}. {display_name:<30} ({size})")
    
    # TÃ­nh tá»•ng kÃ­ch thÆ°á»›c
    total_mb = 0
    for _, (_, size) in MODELS.items():
        if "GB" in size:
            total_mb += float(size.replace("GB", "")) * 1024
        elif "MB" in size:
            total_mb += float(size.replace("MB", ""))
    
    print(f"\nğŸ’¾ Tá»•ng kÃ­ch thÆ°á»›c Æ°á»›c tÃ­nh: ~{total_mb/1024:.2f}GB ({total_mb:.0f}MB)")
    print(f"â±ï¸  Thá»i gian Æ°á»›c tÃ­nh: ~{len(MODELS)*5}-{len(MODELS)*15} phÃºt (tÃ¹y máº¡ng)")
    
    # XÃ¡c nháº­n
    print("\n" + "="*60)
    confirm = input("âš ï¸  Báº¡n cÃ³ muá»‘n tiáº¿p tá»¥c? (y/n): ")
    if confirm.lower() != 'y':
        print("âŒ ÄÃ£ há»§y")
        return
    
    # Táº£i tá»«ng model
    print("\n" + "="*60)
    print("ğŸ¯ Báº®T Äáº¦U Táº¢I")
    print("="*60)
    
    success_count = 0
    total_count = len(MODELS)
    
    for i, (model_name, (display_name, size)) in enumerate(MODELS.items(), 1):
        if download_model(model_name, display_name, size, i, total_count):
            success_count += 1
        
        # In progress tá»•ng thá»ƒ
        print_progress(i, total_count, prefix='ğŸ“Š Tá»•ng tiáº¿n Ä‘á»™:', suffix=f'{i}/{total_count} models')
    
    # Tá»•ng káº¿t
    print("\n\n" + "â•”" + "="*58 + "â•—")
    print("â•‘" + " "*18 + "âœ¨ HOÃ€N THÃ€NH" + " "*27 + "â•‘")
    print("â•š" + "="*58 + "â•")
    
    print(f"\nâœ… ThÃ nh cÃ´ng: {success_count}/{total_count} models")
    
    if success_count < total_count:
        print(f"âš ï¸  Tháº¥t báº¡i: {total_count - success_count} models")
        print("ğŸ’¡ Tip: Cháº¡y láº¡i script Ä‘á»ƒ táº£i cÃ¡c models bá»‹ lá»—i")
    
    print(f"\nğŸ“‚ Vá»‹ trÃ­: {models_dir}")
    
    # Kiá»ƒm tra cáº¥u trÃºc thÆ° má»¥c
    print("\nğŸ“ Cáº¥u trÃºc thÆ° má»¥c:")
    if os.path.exists(models_dir):
        items = sorted(os.listdir(models_dir))
        if items:
            for item in items:
                item_path = os.path.join(models_dir, item)
                if os.path.isdir(item_path):
                    # Äáº¿m sá»‘ file vÃ  tÃ­nh kÃ­ch thÆ°á»›c
                    files = [f for f in os.listdir(item_path) if os.path.isfile(os.path.join(item_path, f))]
                    total_size = sum(
                        os.path.getsize(os.path.join(item_path, f)) 
                        for f in files
                    ) / (1024 * 1024)
                    print(f"   ğŸ“¦ {item}/ ({len(files)} files, {total_size:.1f}MB)")
        else:
            print("   (Trá»‘ng)")
    
    # Hiá»ƒn thá»‹ cÃ¡ch sá»­ dá»¥ng
    print("\n" + "="*60)
    print("ğŸ’¡ CÃCH Sá»¬ Dá»¤NG:")
    print("="*60)
    print("from transformers import AutoTokenizer, AutoModelForSeq2SeqLM")
    print()
    print("# Load model tá»« local (KHÃ”NG Cáº¦N Máº NG)")
    print("tokenizer = AutoTokenizer.from_pretrained('./models/facebook_bart-base')")
    print("model = AutoModelForSeq2SeqLM.from_pretrained('./models/facebook_bart-base')")
    print("="*60)
    
    # ThÃ´ng tin cache (cÃ³ thá»ƒ xÃ³a)
    print("\nğŸ’¡ LÆ°u Ã½:")
    print("   - Models Ä‘Ã£ lÆ°u trong ./models/")
    print("   - Cache táº¡i ~/.cache/huggingface/ cÃ³ thá»ƒ XÃ“A Ä‘Æ°á»£c")
    print("   - Äá»ƒ xÃ³a cache: rm -rf ~/.cache/huggingface/")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ÄÃ£ há»§y bá»Ÿi ngÆ°á»i dÃ¹ng")
        sys.exit(0)
