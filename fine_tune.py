import os
import json
import argparse
from pathlib import Path
from typing import List, Optional
import torch
import torch.nn as nn
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq,
    TrainerCallback,
)
import numpy as np
import logging
import time
from datetime import timedelta

# ============================================================================
# SETUP CUDA T·ªêI ∆ØU CHO RTX 3050 Ti LAPTOP WINDOWS 11
# By HHL Team
# ============================================================================
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
# T·∫Øt warning v·ªÅ torch extensions
os.environ['TORCH_ALLOW_TF32'] = '1'
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
# T·∫Øt distributed warnings
os.environ['NCCL_DEBUG'] = 'INFO'

torch.cuda.empty_cache()

# T·∫Øt cuDNN benchmarking ƒë·ªÉ ·ªïn ƒë·ªãnh
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True

# Setup logging (ch·ªâ INFO ƒë·ªÉ b·ªè DEBUG v√† WARNING d∆∞ th·ª´a)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# T·∫Øt warning t·ª´ transformers v√† torch
import warnings
warnings.filterwarnings('ignore')

# T·∫Øt verbose logs t·ª´ transformers
from transformers.utils import logging as hf_logging
hf_logging.set_verbosity_error()

class ProgressAndNaNCallback(TrainerCallback):
    """Callback ƒë·ªÉ theo d√µi ti·∫øn ƒë·ªô chi ti·∫øt v√† ph√°t hi·ªán/x·ª≠ l√Ω NaN"""
    
    def __init__(self):
        self.start_time = None
        self.loss_history = []
        self.nan_count = 0
        self.last_log_step = 0
    
    def on_train_begin(self, args, state, control, **kwargs):
        """G·ªçi ·ªü ƒë·∫ßu training"""
        self.start_time = time.time()
        self.nan_count = 0
        self.loss_history = []
        print("\n" + "=" * 80)
        print("üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN")
        print("=" * 80 + "\n")
    
    def on_step_end(self, args, state, control, **kwargs):
        """Ki·ªÉm tra NaN v√† hi·ªÉn th·ªã ti·∫øn ƒë·ªô sau m·ªói step"""
        if not state.log_history:
            return control
        
        last_log = state.log_history[-1]
        loss = last_log.get('loss')
        learning_rate = last_log.get('learning_rate', 0)
        
        # Ki·ªÉm tra NaN/Inf trong loss
        if loss is not None and (np.isnan(loss) or np.isinf(loss)):
            print(f"\n‚ùå ERROR: NaN/Inf in loss: {loss}")
            control.should_training_stop = True
            return control
        
        # L∆∞u loss
        if loss is not None:
            self.loss_history.append(loss)
        
        # Hi·ªÉn th·ªã ti·∫øn ƒë·ªô (m·ªói 50 steps)
        if state.global_step % 50 == 0 or state.global_step == 1:
            total_steps = state.max_steps
            progress = state.global_step / total_steps if total_steps > 0 else 0
            
            # T√≠nh th·ªùi gian c√≤n l·∫°i
            elapsed = time.time() - self.start_time
            if progress > 0.01:
                eta_seconds = (elapsed / progress) - elapsed
                eta_str = str(timedelta(seconds=int(eta_seconds)))
            else:
                eta_str = "Calculating..."
            
            # Progress bar
            bar_len = 30
            filled = int(bar_len * progress)
            bar = '‚ñà' * filled + '‚ñë' * (bar_len - filled)
            
            # Print
            print(f"‚è≥ [{state.global_step:5d}/{total_steps}] {bar} "
                  f"{progress*100:5.1f}% | Loss: {loss:.4f} | ETA: {eta_str}")
        
        return control
    
    def on_epoch_end(self, args, state, control, **kwargs):
        """G·ªçi ·ªü cu·ªëi m·ªói epoch"""
        elapsed = time.time() - self.start_time
        elapsed_str = str(timedelta(seconds=int(elapsed)))
        avg_loss = np.mean(self.loss_history[-100:]) if self.loss_history else 0
        
        print(f"\n{'='*80}")
        print(f"‚úÖ EPOCH {int(state.epoch)} HO√ÄN TH√ÄNH")
        print(f"{'='*80}")
        print(f"   ‚è±Ô∏è  Th·ªùi gian: {elapsed_str}")
        print(f"   üìä Step: {state.global_step}/{state.max_steps}")
        print(f"   üìà Loss: {self.loss_history[-1]:.4f} (avg: {avg_loss:.4f})")
        print(f"{'='*80}\n")
    
    def on_train_end(self, args, state, control, **kwargs):
        """G·ªçi ·ªü cu·ªëi training"""
        elapsed = time.time() - self.start_time
        elapsed_str = str(timedelta(seconds=int(elapsed)))
        final_loss = self.loss_history[-1] if self.loss_history else 0
        min_loss = min(self.loss_history) if self.loss_history else 0
        
        print(f"\n{'='*80}")
        print(f"üéâ HU·∫§N LUY·ªÜN HO√ÄN TH√ÄNH!")
        print(f"{'='*80}")
        print(f"   ‚è±Ô∏è  T·ªïng th·ªùi gian: {elapsed_str}")
        print(f"   üìä T·ªïng steps: {state.global_step}")
        print(f"   üìà Final loss: {final_loss:.4f}")
        print(f"   üìä Min loss: {min_loss:.4f}")
        print(f"{'='*80}\n")


class AcademicTextDataset(Dataset):
    """Dataset cho vi·ªác fine-tune model t√≥m t·∫Øt vƒÉn b·∫£n h·ªçc thu·∫≠t"""
    
    def __init__(
        self,
        tokenizer,
        data_file: str,
        max_input_length: int = 512,
        max_target_length: int = 256,
    ):
        self.tokenizer = tokenizer
        self.max_input_length = max_input_length
        self.max_target_length = max_target_length
        self.examples = []
        
        self._load_data(data_file)
    
    def _load_data(self, data_file: str):
        """Load d·ªØ li·ªáu t·ª´ file .jsonl"""
        if not os.path.exists(data_file):
            raise FileNotFoundError(f"File kh√¥ng t·ªìn t·∫°i: {data_file}")
        
        logger.info(f"ƒêang t·∫£i d·ªØ li·ªáu t·ª´: {data_file}")
        count_skipped = 0
        
        with open(data_file, 'r', encoding='utf-8') as f:
            for line_idx, line in enumerate(f, 1):
                try:
                    data = json.loads(line.strip())
                    
                    # Ki·ªÉm tra d·ªØ li·ªáu c·∫ßn thi·∫øt
                    if "full_text" not in data or "abstract" not in data:
                        count_skipped += 1
                        continue
                    
                    full_text = data["full_text"]
                    abstract = data["abstract"]
                    
                    # X·ª≠ l√Ω NaN, None ho·∫∑c ki·ªÉu kh√¥ng ph·∫£i string
                    if full_text is None or abstract is None:
                        count_skipped += 1
                        continue
                    
                    if not isinstance(full_text, str):
                        full_text = str(full_text)
                    if not isinstance(abstract, str):
                        abstract = str(abstract)
                    
                    full_text = full_text.strip()
                    abstract = abstract.strip()
                    
                    # B·ªè qua d·ªØ li·ªáu tr·ªëng ho·∫∑c qu√° ng·∫Øn
                    if not full_text or not abstract or len(full_text) < 10:
                        count_skipped += 1
                        continue
                    
                    self.examples.append({
                        "input_text": full_text,
                        "target_text": abstract
                    })
                    
                except (json.JSONDecodeError, ValueError, TypeError) as e:
                    count_skipped += 1
                    continue
        
        logger.info(f"‚úÖ ƒê√£ t·∫£i {len(self.examples)} v√≠ d·ª• (b·ªè qua {count_skipped})")
        
        if len(self.examples) == 0:
            raise ValueError("‚ùå Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu h·ª£p l·ªá trong file")
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        example = self.examples[idx]
        
        # Encode input
        inputs = self.tokenizer(
            example["input_text"],
            max_length=self.max_input_length,
            padding="max_length",
            truncation=True,
            return_tensors="np"  # Tr·∫£ v·ªÅ numpy array thay v√¨ pt
        )
        
        # Encode target
        targets = self.tokenizer(
            example["target_text"],
            max_length=self.max_target_length,
            padding="max_length",
            truncation=True,
            return_tensors="np"  # Tr·∫£ v·ªÅ numpy array thay v√¨ pt
        )
        
        return {
            "input_ids": inputs["input_ids"][0],
            "attention_mask": inputs["attention_mask"][0],
            "labels": targets["input_ids"][0],
            "decoder_attention_mask": targets["attention_mask"][0],
        }


class AcademicFineTuner:
    """Class qu·∫£n l√Ω vi·ªác fine-tune model t√≥m t·∫Øt vƒÉn b·∫£n h·ªçc thu·∫≠t"""
    
    def __init__(self, base_model_path: Optional[str] = None):
        self.fine_tuned_model_path = "./models/hhlai_academic_textsum"
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        logger.info(f"üîß S·ª≠ d·ª•ng device: {self.device}")
        logger.info(f"üíæ CUDA available: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            logger.info(f"üéÆ GPU: {torch.cuda.get_device_name(0)}")
            logger.info(f"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        
        # Ch·ªçn base model
        self.base_model_path = base_model_path or self._select_base_model()
        self._load_base_model()
    
    def _select_base_model(self) -> str:
        models_dir = "./models"
        available_models = [
            d for d in os.listdir(models_dir)
            if os.path.isdir(os.path.join(models_dir, d))
            and d != "hhlai_academic_textsum"
        ]
        
        if not available_models:
            raise FileNotFoundError("Kh√¥ng t√¨m th·∫•y model n√†o trong ./models/")
        
        print("\nüì¶ Danh s√°ch c√°c model c√≥ s·∫µn:")
        for i, model in enumerate(available_models, 1):
            print(f"   {i}. {model}")
        
        while True:
            try:
                choice = int(input(f"\nCh·ªçn model (1-{len(available_models)}): "))
                if 1 <= choice <= len(available_models):
                    selected = available_models[choice - 1]
                    return os.path.join(models_dir, selected)
            except ValueError:
                pass
            print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá")
    
    def _load_base_model(self):
        logger.info(f"üìÇ ƒêang t·∫£i base model t·ª´: {self.base_model_path}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_path)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(
                self.base_model_path,
                device_map='auto'
            )
            
            # Enable gradient checkpointing ƒë·ªÉ ti·∫øt ki·ªám memory
            self.model.gradient_checkpointing_enable()
            
            logger.info("‚úÖ Base model ƒë√£ load th√†nh c√¥ng")
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi load model: {e}")
            raise
    
    def fine_tune(
        self,
        data_file: str,
        epochs: int = 3,
        batch_size: int = 8,
        learning_rate: float = 5e-5,
        max_input_length: int = 512,
        max_target_length: int = 256,
        warmup_steps: int = 500,
        weight_decay: float = 0.01,
        gradient_accumulation_steps: int = 2,
    ):
        """
        Gi·∫£i th√≠ch th√¥ng s·ªë Fine-tune model
            data_file: ƒê∆∞·ªùng d·∫´n file d·ªØ li·ªáu .jsonl
            epochs: S·ªë epoch hu·∫•n luy·ªán
            batch_size: Batch size
            learning_rate: Learning rate
            max_input_length: ƒê·ªô d√†i t·ªëi ƒëa input
            max_target_length: ƒê·ªô d√†i t·ªëi ƒëa target
            warmup_steps: S·ªë warmup steps
            weight_decay: Weight decay
            gradient_accumulation_steps: S·ªë b∆∞·ªõc t√≠ch l≈©y gradient (tƒÉng hi·ªáu qu·∫£ batch size)
        """
        logger.info("üöÄ B·∫Øt ƒë·∫ßu fine-tune")
        
        # T·∫°o th∆∞ m·ª•c output
        os.makedirs(self.fine_tuned_model_path, exist_ok=True)
        
        # Load dataset
        logger.info("üìä ƒêang chu·∫©n b·ªã d·ªØ li·ªáu...")
        dataset = AcademicTextDataset(
            tokenizer=self.tokenizer,
            data_file=data_file,
            max_input_length=max_input_length,
            max_target_length=max_target_length,
        )
        
        # Hi·ªÉn th·ªã th√¥ng tin training
        total_steps = (len(dataset) // (batch_size * gradient_accumulation_steps)) * epochs
        print(f"\nüìà TH√îNG TIN HU·∫§N LUY·ªÜN:")
        print("=" * 70)
        print(f"   üìù S·ªë l∆∞·ª£ng v√≠ d·ª•: {len(dataset)}")
        print(f"   üîÑ S·ªë epoch: {epochs}")
        print(f"   üì¶ Batch size: {batch_size}")
        print(f"   üì¶ Gradient accumulation: {gradient_accumulation_steps}")
        print(f"   üìä Effective batch size: {batch_size * gradient_accumulation_steps}")
        print(f"   üìä T·ªïng steps: {total_steps}")
        print(f"   üéØ Learning rate: {learning_rate}")
        print(f"   üå°Ô∏è  Device: {self.device}")
        print(f"   üíæ Output: {self.fine_tuned_model_path}")
        print("=" * 70)
        
        # Data collator
        data_collator = DataCollatorForSeq2Seq(
            self.tokenizer,
            model=self.model,
            label_pad_token_id=-100,
        )
        
        # Training arguments - T·ªêI ∆ØU CHO WINDOWS RTX 3050 Ti
        training_args = Seq2SeqTrainingArguments(
            output_dir=self.fine_tuned_model_path,
            num_train_epochs=epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            learning_rate=learning_rate,
            weight_decay=weight_decay,
            warmup_steps=warmup_steps,
            # FP32 - ·ªïn ƒë·ªãnh nh·∫•t cho Windows
            fp16=False,
            bf16=False,
            # Saving
            save_total_limit=1,
            save_steps=1000,
            # Logging - gi·∫£m overhead
            logging_steps=10,
            logging_first_step=False,
            logging_nan_inf_filter=False,
            # Optimization
            optim="adamw_torch",
            eval_strategy="no",
            # Memory optimization
            gradient_checkpointing=True,
            # Data loading (WINDOWS KH√îNG H·ªñ TR·ª¢ NUM_WORKERS)
            dataloader_num_workers=0,  # B·∫ÆT BU·ªòC = 0 cho Windows
            dataloader_pin_memory=False,  # T·∫Øt tr√™n Windows
            # Report
            report_to=[],
            # Seed
            seed=42,
        )
        
        # Trainer v·ªõi Progress v√† NaN callback
        trainer = Seq2SeqTrainer(
            model=self.model,
            args=training_args,
            train_dataset=dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer,
            callbacks=[ProgressAndNaNCallback()],
        )
        
        # Fine-tune
        print("\n‚è≥ ƒêang hu·∫•n luy·ªán...\n")
        start_time = time.time()
        
        try:
            trainer.train()
            
            training_time = time.time() - start_time
            training_time_str = str(timedelta(seconds=int(training_time)))
            
            print(f"\n\n‚úÖ HU·∫§N LUY·ªÜN HO√ÄN TH√ÄNH!")
            print("=" * 70)
            print(f"   ‚è±Ô∏è  Th·ªùi gian: {training_time_str}")
            print(f"   üìä T·ªïng steps: {trainer.state.global_step}")
            print("=" * 70)
            
        except KeyboardInterrupt:
            print("\n\n‚ö†Ô∏è  ƒê√£ d·ª´ng hu·∫•n luy·ªán b·ªüi ng∆∞·ªùi d√πng")
            training_time = time.time() - start_time
            training_time_str = str(timedelta(seconds=int(training_time)))
            print(f"   ‚è±Ô∏è  Th·ªùi gian ch·∫°y: {training_time_str}")
        
        # L∆∞u model
        logger.info(f"üíæ ƒêang l∆∞u model v√†o: {self.fine_tuned_model_path}")
        self.model.save_pretrained(self.fine_tuned_model_path)
        self.tokenizer.save_pretrained(self.fine_tuned_model_path)
        
        print(f"\n‚úÖ FINE-TUNE HO√ÄN TH√ÄNH!")
        print(f"üìÇ Model ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {self.fine_tuned_model_path}")
        
        # Ki·ªÉm tra k√≠ch th∆∞·ªõc model
        model_size = sum(
            os.path.getsize(os.path.join(self.fine_tuned_model_path, f))
            for f in os.listdir(self.fine_tuned_model_path)
            if os.path.isfile(os.path.join(self.fine_tuned_model_path, f))
        ) / (1024 * 1024)
        print(f"üíæ K√≠ch th∆∞·ªõc model: {model_size:.1f}MB\n")


def main():
    # Ph√¢n t√≠ch ƒë·ªëi s·ªë CLI
    parser = argparse.ArgumentParser(
        description="Fine-tune model t√≥m t·∫Øt vƒÉn b·∫£n h·ªçc thu·∫≠t"
    )
    
    parser.add_argument(
        "--base-model",
        type=str,
        help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn base model (VD: ./models/google_mt5-base)"
    )
    
    parser.add_argument(
        "--data-file",
        type=str,
        default="./_crawler/datasets.jsonl",
        help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn file d·ªØ li·ªáu .jsonl"
    )
    
    parser.add_argument(
        "--epochs",
        type=int,
        default=1,
        help="S·ªë epoch hu·∫•n luy·ªán"
    )
    
    parser.add_argument(
        "--batch-size",
        type=int,
        default=8,
        help="Batch size"
    )
    
    parser.add_argument(
        "--learning-rate",
        type=float,
        default=1e-4,
        help="Learning rate"
    )
    
    parser.add_argument(
        "--max-input-length",
        type=int,
        default=512,
        help="ƒê·ªô d√†i t·ªëi ƒëa input"
    )
    
    parser.add_argument(
        "--max-target-length",
        type=int,
        default=256,
        help="ƒê·ªô d√†i t·ªëi ƒëa target"
    )
    
    parser.add_argument(
        "--warmup-steps",
        type=int,
        default=100,
        help="S·ªë warmup steps"
    )
    
    parser.add_argument(
        "--gradient-accumulation",
        type=int,
        default=2,
        help="Gradient accumulation steps"
    )
    
    args = parser.parse_args()
    
    try:
        # Kh·ªüi t·∫°o fine-tuner
        fine_tuner = AcademicFineTuner(base_model_path=args.base_model)
        
        # X√°c nh·∫≠n file d·ªØ li·ªáu
        if not os.path.exists(args.data_file):
            logger.error(f"‚ùå File d·ªØ li·ªáu kh√¥ng t√¨m th·∫•y: {args.data_file}")
            return
        
        # X√°c nh·∫≠n tr∆∞·ªõc khi fine-tune
        print("\n" + "=" * 70)
        print("‚ö†Ô∏è  X√ÅC NH·∫¨N C·∫§U H√åNH:")
        print("=" * 70)
        print(f"üìÇ Base model: {fine_tuner.base_model_path}")
        print(f"üìù File d·ªØ li·ªáu: {args.data_file}")
        print(f"üéØ Output model: {fine_tuner.fine_tuned_model_path}")
        print("=" * 70)
        
        confirm = input("\n‚úÖ B·∫°n c√≥ ch·∫Øc mu·ªën ti·∫øp t·ª•c? (y/n): ").strip().lower()
        if confirm != 'y':
            print("‚ùå ƒê√£ h·ªßy")
            return
        
        # Fine-tune
        fine_tuner.fine_tune(
            data_file=args.data_file,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            max_input_length=args.max_input_length,
            max_target_length=args.max_target_length,
            warmup_steps=args.warmup_steps,
            gradient_accumulation_steps=args.gradient_accumulation,
        )
    
    except FileNotFoundError as e:
        logger.error(f"‚ùå {e}")
        return
    
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  ƒê√£ d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
        return
    
    except Exception as e:
        logger.error(f"‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
    