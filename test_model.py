import os
import sys
import torch
import argparse
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ModelTester:
    """Class Ä‘á»ƒ test model tÃ³m táº¯t vÄƒn báº£n"""
    
    def __init__(self, model_path: str = "./models/hhlai_academic_textsum"):
        """
        Khá»Ÿi táº¡o model tester
        
        Args:
            model_path: ÄÆ°á»ng dáº«n Ä‘áº¿n model fine-tuned
        """
        self.model_path = model_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        logger.info(f"ğŸ”§ Sá»­ dá»¥ng device: {self.device}")
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"âŒ Model khÃ´ng tÃ¬m tháº¥y táº¡i: {model_path}")
        
        self._load_model()
    
    def _load_model(self):
        """Load model Ä‘Ã£ fine-tune"""
        logger.info(f"ğŸ“‚ Äang táº£i model tá»«: {self.model_path}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_path)
            
            # Load vá»›i fp16 náº¿u cÃ³ CUDA
            if torch.cuda.is_available():
                self.model = self.model.half()
            
            self.model.to(self.device)
            self.model.eval()
            
            logger.info("âœ… Model Ä‘Ã£ load thÃ nh cÃ´ng\n")
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi load model: {e}")
            raise
    
    def summarize(
        self,
        text: str,
        max_length: int = 256,
        min_length: int = 50,
        num_beams: int = 4,
        language: str = "en",
    ) -> str:
        """
        TÃ³m táº¯t vÄƒn báº£n
        
        Args:
            text: VÄƒn báº£n cáº§n tÃ³m táº¯t
            max_length: Äá»™ dÃ i tá»‘i Ä‘a cá»§a tÃ³m táº¯t
            min_length: Äá»™ dÃ i tá»‘i thiá»ƒu cá»§a tÃ³m táº¯t
            num_beams: Sá»‘ beams cho beam search
            language: NgÃ´n ngá»¯ ("en" hoáº·c "vi")
        
        Returns:
            VÄƒn báº£n tÃ³m táº¯t
        """
        # ThÃªm prefix ngÃ´n ngá»¯
        prefix = "summarize: " if language == "en" else "tÃ³m táº¯t: "
        input_text = prefix + text.strip()
        
        # Tokenize
        inputs = self.tokenizer(
            input_text,
            max_length=512,
            truncation=True,
            return_tensors="pt"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                min_length=min_length,
                num_beams=num_beams,
                early_stopping=True,
                no_repeat_ngram_size=2,
            )
        
        # Decode
        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return summary


def main():
    parser = argparse.ArgumentParser(
        description="Test model tÃ³m táº¯t vÄƒn báº£n há»c thuáº­t"
    )
    
    parser.add_argument(
        "--model-path",
        type=str,
        default="./models/hhlai_academic_textsum",
        help="ÄÆ°á»ng dáº«n Ä‘áº¿n model fine-tuned"
    )
    
    parser.add_argument(
        "--language",
        choices=["en", "vi"],
        default="en",
        help="NgÃ´n ngá»¯ (en: tiáº¿ng Anh, vi: tiáº¿ng Viá»‡t)"
    )
    
    parser.add_argument(
        "--max-length",
        type=int,
        default=256,
        help="Äá»™ dÃ i tá»‘i Ä‘a cá»§a tÃ³m táº¯t"
    )
    
    parser.add_argument(
        "--min-length",
        type=int,
        default=50,
        help="Äá»™ dÃ i tá»‘i thiá»ƒu cá»§a tÃ³m táº¯t"
    )
    
    parser.add_argument(
        "--num-beams",
        type=int,
        default=4,
        help="Sá»‘ beams cho beam search"
    )
    
    args = parser.parse_args()
    
    try:
        # Khá»Ÿi táº¡o tester
        tester = ModelTester(model_path=args.model_path)
        
        # Hiá»ƒn thá»‹ hÆ°á»›ng dáº«n
        lang_name = "Tiáº¿ng Anh" if args.language == "en" else "Tiáº¿ng Viá»‡t"
        print("=" * 80)
        print("ğŸ§ª Há»† THá»NG TEST TÃ“M Táº®T VÄ‚N Báº¢N Há»ŒC THUáº¬T")
        print("=" * 80)
        print(f"ğŸ“š NgÃ´n ngá»¯: {lang_name}")
        print(f"âš™ï¸  Max length: {args.max_length}, Min length: {args.min_length}")
        print(f"ğŸ”— Model: {args.model_path}")
        print("=" * 80)
        print("\nğŸ’¡ HÆ¯á»šNG DáºªN:")
        print("  â€¢ Nháº­p vÄƒn báº£n cáº§n tÃ³m táº¯t")
        print("  â€¢ Nháº¥n Enter 2 láº§n Ä‘á»ƒ káº¿t thÃºc nháº­p liá»‡u")
        print("  â€¢ Nháº­p 'exit' Ä‘á»ƒ thoÃ¡t chÆ°Æ¡ng trÃ¬nh")
        print("\n" + "=" * 80 + "\n")
        
        # VÃ²ng láº·p nháº­p vÃ  xá»­ lÃ½
        while True:
            # Nháº­p vÄƒn báº£n tá»« bÃ n phÃ­m
            print("ğŸ“ Nháº­p vÄƒn báº£n (Nháº¥n Enter 2 láº§n Ä‘á»ƒ hoÃ n thÃ nh):")
            print("-" * 80)
            
            lines = []
            empty_count = 0
            
            while empty_count < 2:
                try:
                    line = input()
                    if line.strip() == "":
                        empty_count += 1
                    else:
                        empty_count = 0
                        lines.append(line)
                except EOFError:
                    break
            
            text = " ".join(lines).strip()
            
            # Kiá»ƒm tra lá»‡nh Ä‘áº·c biá»‡t
            if text.lower() == "exit":
                print("\nğŸ‘‹ Cáº£m Æ¡n Ä‘Ã£ sá»­ dá»¥ng! Táº¡m biá»‡t!")
                break
            
            if not text:
                print("âš ï¸  VÄƒn báº£n khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng!\n")
                continue
            
            # TÃ³m táº¯t vÄƒn báº£n
            print("\nâ³ Äang tÃ³m táº¯t...")
            try:
                summary = tester.summarize(
                    text=text,
                    max_length=args.max_length,
                    min_length=args.min_length,
                    num_beams=args.num_beams,
                    language=args.language,
                )
                
                # Hiá»ƒn thá»‹ káº¿t quáº£
                print("\n" + "=" * 80)
                print("ğŸ“„ VÄ‚N Báº¢N Gá»C:")
                print("-" * 80)
                print(text)
                print("\n" + "=" * 80)
                print("âœ… TÃ“M Táº®T:")
                print("-" * 80)
                print(summary)
                print("=" * 80 + "\n")
                
            except Exception as e:
                logger.error(f"âŒ Lá»—i khi tÃ³m táº¯t: {e}")
                print("\n")
                continue
    
    except FileNotFoundError as e:
        print(f"âŒ {e}")
        print("ğŸ’¡ HÃ£y cháº¡y fine_tune_for_academic_text_summary.py trÆ°á»›c!")
        sys.exit(1)
    
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ÄÃ£ dá»«ng chÆ°Æ¡ng trÃ¬nh. Táº¡m biá»‡t!")
        sys.exit(0)
    
    except Exception as e:
        logger.error(f"âŒ Lá»—i: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()