"""
Model Setup Helper - Ki·ªÉm tra v√† setup model c·ª•c b·ªô
"""
import os
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM


def check_local_model(model_path: str = "./models/hhlai_academic_textsum") -> bool:
    """
    Ki·ªÉm tra xem model c·ª•c b·ªô c√≥ t·ªìn t·∫°i v√† ho√†n ch·ªânh kh√¥ng
    
    Args:
        model_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c model
        
    Returns:
        True n·∫øu model c√≥ s·∫µn, False n·∫øu kh√¥ng
    """
    model_dir = Path(model_path)
    
    if not model_dir.exists():
        print(f"‚ùå Model directory not found: {model_path}")
        return False
    
    # Ki·ªÉm tra c√°c file c·∫ßn thi·∫øt
    required_files = [
        "config.json",
        "model.safetensors",  # ho·∫∑c pytorch_model.bin
        "tokenizer.model"     # ho·∫∑c tokenizer.json
    ]
    
    # Ho·∫∑c ki·ªÉm tra pytorch_model.bin thay v√¨ model.safetensors
    alternative_files = ["pytorch_model.bin", "tokenizer.json"]
    
    files_found = []
    for file in (model_dir / file for file in required_files if (model_dir / file).exists()):
        files_found.append(file.name)
    
    # Ki·ªÉm tra alternative files
    for file in alternative_files:
        if (model_dir / file).exists():
            files_found.append(file)
    
    print(f"üìÅ Model directory: {model_path}")
    print(f"üìÑ Files found: {len(files_found)}")
    
    if (model_dir / "config.json").exists():
        print(f"‚úÖ Model config found")
        return True
    else:
        print(f"‚ùå Model config not found (config.json missing)")
        return False


def setup_local_model(model_path: str = "./models/hhlai_academic_textsum") -> bool:
    """
    Setup model c·ª•c b·ªô - T·∫°o th∆∞ m·ª•c n·∫øu kh√¥ng t·ªìn t·∫°i
    
    Args:
        model_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c model
        
    Returns:
        True n·∫øu setup th√†nh c√¥ng
    """
    model_dir = Path(model_path)
    
    # T·∫°o th∆∞ m·ª•c n·∫øu kh√¥ng t·ªìn t·∫°i
    model_dir.mkdir(parents=True, exist_ok=True)
    print(f"‚úÖ Model directory ready: {model_path}")
    
    return True


def download_and_setup_model(
    model_name: str = "hhlai/hhlai_academic_textsum",
    model_path: str = "./models/hhlai_academic_textsum",
    force_download: bool = False
) -> bool:
    """
    Download model t·ª´ HuggingFace v√† l∆∞u c·ª•c b·ªô
    
    Args:
        model_name: Model name t·ª´ HuggingFace
        model_path: ƒê∆∞·ªùng d·∫´n l∆∞u c·ª•c b·ªô
        force_download: Force re-download n·∫øu ƒë√£ t·ªìn t·∫°i
        
    Returns:
        True n·∫øu download th√†nh c√¥ng
    """
    model_dir = Path(model_path)
    
    # Ki·ªÉm tra xem model ƒë√£ t·ªìn t·∫°i ch∆∞a
    if not force_download and check_local_model(model_path):
        print(f"‚úÖ Model already exists locally: {model_path}")
        return True
    
    try:
        # T·∫°o th∆∞ m·ª•c
        setup_local_model(model_path)
        
        print(f"\nüì• Downloading model: {model_name}")
        print(f"üìç Saving to: {model_path}")
        print("‚è≥ This may take several minutes...")
        
        # Download tokenizer
        print("\n1Ô∏è‚É£ Downloading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokenizer.save_pretrained(model_path)
        print(f"‚úÖ Tokenizer saved to {model_path}")
        
        # Download model
        print("\n2Ô∏è‚É£ Downloading model...")
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        model.save_pretrained(model_path)
        print(f"‚úÖ Model saved to {model_path}")
        
        print("\n‚úÖ Model setup completed successfully!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Error downloading model: {str(e)}")
        return False


def verify_model_files(model_path: str = "./models/hhlai_academic_textsum") -> dict:
    """
    Ki·ªÉm tra v√† li·ªát k√™ c√°c file trong th∆∞ m·ª•c model
    
    Args:
        model_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c model
        
    Returns:
        Dictionary v·ªõi th√¥ng tin files
    """
    model_dir = Path(model_path)
    
    info = {
        "path": str(model_dir),
        "exists": model_dir.exists(),
        "files": [],
        "size_mb": 0
    }
    
    if not model_dir.exists():
        return info
    
    # List all files
    total_size = 0
    for file in model_dir.rglob("*"):
        if file.is_file():
            size = file.stat().st_size
            total_size += size
            info["files"].append({
                "name": file.name,
                "relative_path": str(file.relative_to(model_dir)),
                "size_mb": round(size / (1024 * 1024), 2)
            })
    
    info["size_mb"] = round(total_size / (1024 * 1024), 2)
    
    return info


def print_model_info(model_path: str = "./models/hhlai_academic_textsum"):
    """
    In th√¥ng tin chi ti·∫øt v·ªÅ model
    
    Args:
        model_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c model
    """
    info = verify_model_files(model_path)
    
    print("\n" + "="*60)
    print("üìä MODEL INFORMATION")
    print("="*60)
    print(f"Path: {info['path']}")
    print(f"Exists: {'‚úÖ Yes' if info['exists'] else '‚ùå No'}")
    print(f"Total Size: {info['size_mb']} MB")
    print(f"Files Count: {len(info['files'])}")
    
    if info['files']:
        print("\nFiles:")
        for file_info in sorted(info['files'], key=lambda x: x['size_mb'], reverse=True)[:10]:
            print(f"  - {file_info['name']}: {file_info['size_mb']} MB")
    
    print("="*60 + "\n")


if __name__ == "__main__":
    # Test script - ch·∫°y ƒë·ªÉ ki·ªÉm tra model
    
    MODEL_PATH = "./models/hhlai_academic_textsum"
    MODEL_NAME = "hhlai/hhlai_academic_textsum"
    
    print("\nüîç Checking model setup...\n")
    
    # 1. Ki·ªÉm tra model c·ª•c b·ªô
    print("1Ô∏è‚É£ Checking local model...")
    if check_local_model(MODEL_PATH):
        print("\n‚úÖ Local model found!")
        print_model_info(MODEL_PATH)
    else:
        print("\n‚ö†Ô∏è Local model not found")
        print("\n2Ô∏è‚É£ Attempting to download model...")
        if download_and_setup_model(MODEL_NAME, MODEL_PATH):
            print("\n‚úÖ Model downloaded successfully!")
            print_model_info(MODEL_PATH)
        else:
            print("\n‚ùå Failed to download model")
            